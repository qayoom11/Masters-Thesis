{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification(max-labels-intense selection).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vErWpItXGV6p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "831e9469-2628-4367-ef1d-fcc2e9b4d5ef"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6amD-Y3XGZzH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4695cdac-5dc8-4b3c-e44b-f2d5edd540e1"
      },
      "source": [
        "import json\n",
        "from time import time\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, ngrams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_QIARzMGvS7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "9c3700ba-3bf9-4442-be17-bbdf7e15b442"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.feature_extraction import DictVectorizer \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.over_sampling import SVMSMOTE, BorderlineSMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.combine import SMOTEENN\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2,mutual_info_classif\n",
        "from sklearn.svm import SVC, libsvm\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import DistanceMetric\n",
        "from sklearn.linear_model import LogisticRegressionCV"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.svm.libsvm module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.svm. Anything that cannot be imported from sklearn.svm is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F8kphDOGzuX"
      },
      "source": [
        "pooling_function = 'max'\n",
        "with open('/content/drive/My Drive/'+pooling_function+'-labels-intense-selection.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "word_freq = {}\n",
        "with open('/content/drive/My Drive/unigram_freq.csv', 'r', encoding='utf-8') as file:\n",
        "    count = 1\n",
        "    for line in file:\n",
        "      line  = line.strip().split(',')\n",
        "      word_freq[line[0]] = count\n",
        "      count += 1\n",
        "      if count == 17000:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSg3la6bHjKH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcf9b919-2ecd-42b3-cc2d-64e7a4080502"
      },
      "source": [
        "def tokenize(string):\n",
        "  return string.split()\n",
        "\n",
        "y = np.array([x[1] for x in data])\n",
        "print(len(y))\n",
        "\n",
        "start = time()\n",
        "n1gram = 1\n",
        "n2gram = 1\n",
        "#v = DictVectorizer(sparse=True)\n",
        "#v = CountVectorizer(ngram_range=(n1gram,n2gram))\n",
        "v = TfidfVectorizer(ngram_range=(n1gram,n2gram), lowercase=False, sublinear_tf=True, tokenizer=tokenize, stop_words='english')\n",
        "docs = [x[0] + ' '.join([j[1] for j in nltk.pos_tag(x[0].split())]) for x in data]\n",
        "X_ = v.fit_transform(docs)\n",
        "end = time()\n",
        "tfidf_scores = v.vocabulary_end = time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F1ThSMhHt5P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f8d119d-aeea-47ef-a08c-108c2ff0e47e"
      },
      "source": [
        "X_.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 54992)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntepgloIKwJE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "d24ff7ac-1f71-4889-bad1-c5be2db28364"
      },
      "source": [
        "print('preprocessing time', round(end-start, 4))\n",
        "\n",
        "kf = StratifiedKFold(n_splits = 10)\n",
        "n_estimators = 10\n",
        "clf = SVC(max_iter=10000)\n",
        "\n",
        "accuracy = 0\n",
        "balance = 0\n",
        "recall = 0\n",
        "precision = 0\n",
        "specificity = 0\n",
        "fmeasure = 0\n",
        "TP = TN = FP = FN = 0\n",
        "classification_time = 0\n",
        "for train_index, test_index in kf.split(X_, y):\n",
        "    X_train, X_test = X_[train_index], X_[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    startc = time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_predicted = clf.predict(X_test)\n",
        "    endc = time()\n",
        "    classification_time += endc-startc\n",
        "    #print('Confusion Matrix')\n",
        "    #print(metrics.confusion_matrix(y_test, y_predicted))\n",
        "    tn, fp, fn, tp = tuple(metrics.confusion_matrix(y_test, y_predicted).flatten())\n",
        "    TP += tp\n",
        "    TN += tn\n",
        "    FP += fp\n",
        "    FN += fn\n",
        "    \n",
        "    #print('Classification Reprot')\n",
        "    #print(metrics.classification_report(y_test, y_predicted))\n",
        "\n",
        "    #print('Accuracy')\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "    accuracy += acc\n",
        "    \n",
        "    pre = tp / (tp + fp)\n",
        "    precision += pre\n",
        "    \n",
        "    rec = tp / (tp + fn)\n",
        "    recall += rec\n",
        "    \n",
        "    spe = tn / (tn + fp)\n",
        "    specificity += spe\n",
        "    \n",
        "    fm = (2 * pre * rec) / (pre + rec)\n",
        "    fmeasure += fm\n",
        "\n",
        "    bl = (rec + spe) / 2.\n",
        "    balance += bl \n",
        "\n",
        "print('classification time', round(classification_time/10., 4))\n",
        "\n",
        "print('TPR Recall', round(recall/10., 4))\n",
        "print('TNR specificity', round(specificity/10.,4))\n",
        "print('precision', round(precision/10.,4))\n",
        "print('fmeasure', round(fmeasure/10.,4))\n",
        "print('accuracy', round(accuracy/10.,4))\n",
        "print('bal acc', round(balance/10.,4))\n",
        "print('---------------------------------------------')\n",
        "print(round(recall/10., 4), round(specificity/10.,4), round(precision/10.,4), round(fmeasure/10.,4), round(accuracy/10.,4), round(balance/10.,4))\n",
        "print('---------------------------------------------')\n",
        "print('tp', TP/10.)\n",
        "print('fp', FP/10.)\n",
        "print('fn', FN/10.)\n",
        "print('tn', TN/10.)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing time 18.7746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "classification time 86.3552\n",
            "TPR Recall 0.7881\n",
            "TNR specificity 0.7345\n",
            "precision 0.7482\n",
            "fmeasure 0.7675\n",
            "accuracy 0.7613\n",
            "bal acc 0.7613\n",
            "---------------------------------------------\n",
            "0.7881 0.7345 0.7482 0.7675 0.7613 0.7613\n",
            "---------------------------------------------\n",
            "tp 788.1\n",
            "fp 265.5\n",
            "fn 211.9\n",
            "tn 734.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlos1nWuhpzF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "03274a1b-afc2-4637-c237-396bf2d2639d"
      },
      "source": [
        "print('preprocessing time', round(end-start, 4))\n",
        "\n",
        "kf = StratifiedKFold(n_splits = 10)\n",
        "n_estimators = 10\n",
        "clf = SGDClassifier(max_iter=2000, tol=1e-5)\n",
        "\n",
        "accuracy = 0\n",
        "balance = 0\n",
        "recall = 0\n",
        "precision = 0\n",
        "specificity = 0\n",
        "fmeasure = 0\n",
        "TP = TN = FP = FN = 0\n",
        "classification_time = 0\n",
        "for train_index, test_index in kf.split(X_, y):\n",
        "    X_train, X_test = X_[train_index], X_[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    startc = time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_predicted = clf.predict(X_test)\n",
        "    endc = time()\n",
        "    classification_time += endc-startc\n",
        "    #print('Confusion Matrix')\n",
        "    #print(metrics.confusion_matrix(y_test, y_predicted))\n",
        "    tn, fp, fn, tp = tuple(metrics.confusion_matrix(y_test, y_predicted).flatten())\n",
        "    TP += tp\n",
        "    TN += tn\n",
        "    FP += fp\n",
        "    FN += fn\n",
        "    \n",
        "    #print('Classification Reprot')\n",
        "    #print(metrics.classification_report(y_test, y_predicted))\n",
        "\n",
        "    #print('Accuracy')\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "    accuracy += acc\n",
        "    \n",
        "    pre = tp / (tp + fp)\n",
        "    precision += pre\n",
        "    \n",
        "    rec = tp / (tp + fn)\n",
        "    recall += rec\n",
        "    \n",
        "    spe = tn / (tn + fp)\n",
        "    specificity += spe\n",
        "    \n",
        "    fm = (2 * pre * rec) / (pre + rec)\n",
        "    fmeasure += fm\n",
        "\n",
        "    bl = (rec + spe) / 2.\n",
        "    balance += bl \n",
        "\n",
        "print('classification time', round(classification_time/10., 4))\n",
        "\n",
        "print('TPR Recall', round(recall/10., 4))\n",
        "print('TNR specificity', round(specificity/10.,4))\n",
        "print('precision', round(precision/10.,4))\n",
        "print('fmeasure', round(fmeasure/10.,4))\n",
        "print('accuracy', round(accuracy/10.,4))\n",
        "print('bal acc', round(balance/10.,4))\n",
        "print('---------------------------------------------')\n",
        "print(round(recall/10., 4), round(specificity/10.,4), round(precision/10.,4), round(fmeasure/10.,4), round(accuracy/10.,4), round(balance/10.,4))\n",
        "print('---------------------------------------------')\n",
        "print('tp', TP/10.)\n",
        "print('fp', FP/10.)\n",
        "print('fn', FN/10.)\n",
        "print('tn', TN/10.)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing time 18.7746\n",
            "classification time 0.5675\n",
            "TPR Recall 0.7873\n",
            "TNR specificity 0.7328\n",
            "precision 0.7468\n",
            "fmeasure 0.7664\n",
            "accuracy 0.76\n",
            "bal acc 0.76\n",
            "---------------------------------------------\n",
            "0.7873 0.7328 0.7468 0.7664 0.76 0.76\n",
            "---------------------------------------------\n",
            "tp 787.3\n",
            "fp 267.2\n",
            "fn 212.7\n",
            "tn 732.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e99DkLTdBMi"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcUivKAy66PN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "ed306d1c-211c-44a9-c107-9a718020c6ef"
      },
      "source": [
        "print('preprocessing time', round(end-start, 4))\n",
        "\n",
        "kf = StratifiedKFold(n_splits = 10)\n",
        "n_estimators = 10\n",
        "clf = KNeighborsClassifier(n_neighbors=200)\n",
        "\n",
        "accuracy = 0\n",
        "balance = 0\n",
        "recall = 0\n",
        "precision = 0\n",
        "specificity = 0\n",
        "fmeasure = 0\n",
        "TP = TN = FP = FN = 0\n",
        "classification_time = 0\n",
        "for train_index, test_index in kf.split(X_, y):\n",
        "    X_train, X_test = X_[train_index], X_[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    startc = time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_predicted = clf.predict(X_test)\n",
        "    endc = time()\n",
        "    classification_time += endc-startc\n",
        "    #print('Confusion Matrix')\n",
        "    #print(metrics.confusion_matrix(y_test, y_predicted))\n",
        "    tn, fp, fn, tp = tuple(metrics.confusion_matrix(y_test, y_predicted).flatten())\n",
        "    TP += tp\n",
        "    TN += tn\n",
        "    FP += fp\n",
        "    FN += fn\n",
        "    \n",
        "    #print('Classification Reprot')\n",
        "    #print(metrics.classification_report(y_test, y_predicted))\n",
        "\n",
        "    #print('Accuracy')\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "    accuracy += acc\n",
        "    \n",
        "    pre = tp / (tp + fp)\n",
        "    precision += pre\n",
        "    \n",
        "    rec = tp / (tp + fn)\n",
        "    recall += rec\n",
        "    \n",
        "    spe = tn / (tn + fp)\n",
        "    specificity += spe\n",
        "    \n",
        "    fm = (2 * pre * rec) / (pre + rec)\n",
        "    fmeasure += fm\n",
        "\n",
        "    bl = (rec + spe) / 2.\n",
        "    balance += bl \n",
        "\n",
        "print('classification time', round(classification_time/10., 4))\n",
        "\n",
        "print('TPR Recall', round(recall/10., 4))\n",
        "print('TNR specificity', round(specificity/10.,4))\n",
        "print('precision', round(precision/10.,4))\n",
        "print('fmeasure', round(fmeasure/10.,4))\n",
        "print('accuracy', round(accuracy/10.,4))\n",
        "print('bal acc', round(balance/10.,4))\n",
        "print('---------------------------------------------')\n",
        "print(round(recall/10., 4), round(specificity/10.,4), round(precision/10.,4), round(fmeasure/10.,4), round(accuracy/10.,4), round(balance/10.,4))\n",
        "print('---------------------------------------------')\n",
        "print('tp', TP/10.)\n",
        "print('fp', FP/10.)\n",
        "print('fn', FN/10.)\n",
        "print('tn', TN/10.)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing time 18.7746\n",
            "classification time 2.0717\n",
            "TPR Recall 0.7483\n",
            "TNR specificity 0.6714\n",
            "precision 0.6949\n",
            "fmeasure 0.7206\n",
            "accuracy 0.7098\n",
            "bal acc 0.7098\n",
            "---------------------------------------------\n",
            "0.7483 0.6714 0.6949 0.7206 0.7098 0.7098\n",
            "---------------------------------------------\n",
            "tp 748.3\n",
            "fp 328.6\n",
            "fn 251.7\n",
            "tn 671.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ME82VjV_Tb7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}